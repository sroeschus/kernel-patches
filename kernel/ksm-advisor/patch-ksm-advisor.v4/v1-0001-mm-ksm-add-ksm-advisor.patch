From fb437fb4b844fbffa23ea92f3122299e116bc4eb Mon Sep 17 00:00:00 2001
From: Stefan Roesch <shr@devkernel.io>
Date: Wed, 13 Sep 2023 09:35:20 -0700
Subject: [PATCH v1 1/4] mm/ksm: add ksm advisor

This adds the ksm advisor. The ksm advisor automatically manages the
pages_to_scan setting to achieve a target scan rate. The target scan
time defines how many seconds it should take to scan all the candidate
KSM pages. In other words the pages_to_scan rate is changed by the
advisor to achieve the target scan time. The algorithm has a max and min
value to:
- guarantee responsiveness to changes
- to avoid to spend too much CPU

The respective parameters are:
- ksm_advisor_target_scan_time
- ksm_advisor_min_pages
- ksm_advisor_max_pages.

The algorithm calculates the change value based on the target scan time
and the previous scan time. To avoid pertubations of the changes an
exponentially weighted moving average is used.

By default the advisor is disabled. Currently there are two advisors:
none and scan_time.

Tests with various workloads have shown considerable CPU savings. Most
of the workloads I have investigated have more candidate pages during
startup, once the workload is stable in terms of memory, the number of
candidate pages is reduced. Without the advisor, the pages_to_scan needs
to be sized for the maximum number of candidate pages. So having this
advisor definitely helps in reducing CPU consumption.

For the instagram workload, the advisor achieves a 25% CPU reduction.
Once the memory is stable, the pages_to_scan parameter gets reduced to
about 40% of its max value.

Signed-off-by: Stefan Roesch <shr@devkernel.io>
---
 mm/ksm.c | 98 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 1 file changed, 98 insertions(+)

diff --git a/mm/ksm.c b/mm/ksm.c
index 728574a3033e..e239261d47fe 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -296,6 +296,100 @@ unsigned long ksm_zero_pages;
 /* The number of pages that have been skipped due to "smart scanning" */
 static unsigned long ksm_pages_skipped;
 
+/* At least scan this many pages per cycle. */
+static unsigned long ksm_advisor_min_pages = 500;
+
+/* Don't scan more than max pages per cycle. */
+static unsigned long ksm_advisor_max_pages = 5000;
+
+/* Target time in seconds to analyze all KSM candidate pages. */
+static unsigned long ksm_advisor_target_scan_time = 200;
+
+/**
+ * struct advisor_ctx - metadata for KSM advisor
+ * @start_scan: start time of the current scan
+ * @scan_time: scan time of previous scan
+ * @change: last change in pages_to_scan parameter
+ */
+struct advisor_ctx {
+	ktime_t start_scan;
+	s64	scan_time;
+	unsigned long change;
+};
+static struct advisor_ctx advisor_ctx;
+
+/* Define different advisor's */
+enum ksm_advisor_type {
+	KSM_ADVISOR_NODE,
+	KSM_ADVISOR_SCAN_TIME };
+static enum ksm_advisor_type ksm_advisor;
+
+static void init_advisor(void)
+{
+	advisor_ctx.start_scan = 0;
+	advisor_ctx.scan_time = 0;
+	advisor_ctx.change = 0;
+}
+
+/* Use previous scan time if available, otherwise use current scan time as an
+ * approximation for the previous scan time.
+ */
+static inline s64 prev_scan_time(struct advisor_ctx *ctx, s64 new_scan_time)
+{
+	return ctx->scan_time ? ctx->scan_time : new_scan_time;
+}
+
+/* Exponentially weighted moving average. */
+#define EWMA_WEIGHT 50
+
+static unsigned long ewma(unsigned long prev, unsigned long curr)
+{
+	return ((100 - EWMA_WEIGHT) * prev + EWMA_WEIGHT * curr) / 100;
+}
+
+static void scan_time_advisor(s64 scan_time)
+{
+	unsigned long pages;
+	unsigned long factor;
+	unsigned long change;
+	unsigned long last_scan_time;
+
+	pages           = ksm_thread_pages_to_scan;
+	factor          = ksm_advisor_target_scan_time * 100 / scan_time;
+	factor          = factor ? factor : 1;
+
+	last_scan_time	= prev_scan_time(&advisor_ctx, scan_time);
+	change          = scan_time * 100 / last_scan_time;
+	change          = change ? change : 1;
+	change		= ewma(advisor_ctx.change, change);
+
+	/* Calculate new scan rate based on target scan rate. */
+	pages = pages * 100 / factor;
+	/* Add weighted change percentage to target scan rate. */
+	pages = pages * change / 100;
+
+	/* Cap new page value */
+	pages = max(pages, ksm_advisor_min_pages);
+	pages = min(pages, ksm_advisor_max_pages);
+
+	advisor_ctx.change = change;
+	advisor_ctx.scan_time = scan_time;
+	ksm_thread_pages_to_scan = pages;
+}
+
+static void run_advisor(void)
+{
+	if (ksm_advisor == KSM_ADVISOR_SCAN_TIME) {
+		s64 scan_time;
+
+		scan_time = ktime_ms_delta(ktime_get(), advisor_ctx.start_scan);
+		scan_time /= MSEC_PER_SEC;
+		scan_time = scan_time ? scan_time : 1;
+
+		scan_time_advisor(scan_time);
+	}
+}
+
 #ifdef CONFIG_NUMA
 /* Zeroed when merging across nodes is not allowed */
 static unsigned int ksm_merge_across_nodes = 1;
@@ -2372,6 +2466,7 @@ static struct ksm_rmap_item *scan_get_next_rmap_item(struct page **page)
 
 	mm_slot = ksm_scan.mm_slot;
 	if (mm_slot == &ksm_mm_head) {
+		advisor_ctx.start_scan = ktime_get();
 		trace_ksm_start_scan(ksm_scan.seqnr, ksm_rmap_items);
 
 		/*
@@ -2529,6 +2624,8 @@ static struct ksm_rmap_item *scan_get_next_rmap_item(struct page **page)
 	if (mm_slot != &ksm_mm_head)
 		goto next_mm;
 
+	run_advisor();
+
 	trace_ksm_stop_scan(ksm_scan.seqnr, ksm_rmap_items);
 	ksm_scan.seqnr++;
 	return NULL;
@@ -3574,6 +3671,7 @@ static int __init ksm_init(void)
 	zero_checksum = calc_checksum(ZERO_PAGE(0));
 	/* Default to false for backwards compatibility */
 	ksm_use_zero_pages = false;
+	init_advisor();
 
 	err = ksm_slab_init();
 	if (err)

base-commit: 15bcc9730fcd7526a3b92eff105d6701767a53bb
prerequisite-patch-id: 320dada8f8bd0347a671a502fc8fa5c8888bd69d
prerequisite-patch-id: ca499e78bfde37e0492b84070ed8aaac254cc351
prerequisite-patch-id: 73ad16aa4d6ac6ae87406f1bb3f44d9fa61a7b43
prerequisite-patch-id: d104c7aa5f34fcf7da44d1c051c32a89958f6a29
-- 
2.39.3

