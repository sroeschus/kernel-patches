From 79d1c2ad1db77b1cf97368ae57276cc4593b9637 Mon Sep 17 00:00:00 2001
From: Jens Axboe <axboe@kernel.dk>
Date: Thu, 31 Mar 2022 21:01:14 -0600
Subject: [PATCH v4 4/9] io_uring: wire up inline completion path for CQE32

Rather than always use the slower locked path, wire up use of the
deferred completion path that normal CQEs can take. This reuses the
hash list node for the storage we need to hold the two 64-bit values
that must be passed back.

Signed-off-by: Jens Axboe <axboe@kernel.dk>
---
 fs/io_uring.c | 38 +++++++++++++++++++++++++++++++++-----
 1 file changed, 33 insertions(+), 5 deletions(-)

diff --git a/fs/io_uring.c b/fs/io_uring.c
index cec6fc861620..e2a07cef649c 100644
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@ -936,7 +936,13 @@ struct io_kiocb {
 	atomic_t			poll_refs;
 	struct io_task_work		io_task_work;
 	/* for polled requests, i.e. IORING_OP_POLL_ADD and async armed poll */
-	struct hlist_node		hash_node;
+	union {
+		struct hlist_node	hash_node;
+		struct {
+			u64		extra1;
+			u64		extra2;
+		};
+	};
 	/* internal polling, see IORING_FEAT_FAST_POLL */
 	struct async_poll		*apoll;
 	/* opcode allocated if it needs to store data for async defer */
@@ -2078,8 +2084,8 @@ static noinline bool io_fill_cqe_aux(struct io_ring_ctx *ctx, u64 user_data,
 	return __io_fill_cqe(ctx, user_data, res, cflags);
 }
 
-static void io_fill_cqe32_req(struct io_kiocb *req, s32 res, u32 cflags,
-			      u64 extra1, u64 extra2)
+static void __io_fill_cqe32_req(struct io_kiocb *req, s32 res, u32 cflags,
+				u64 extra1, u64 extra2)
 {
 	struct io_ring_ctx *ctx = req->ctx;
 	struct io_uring_cqe *cqe;
@@ -2157,7 +2163,7 @@ static void io_req_complete_post32(struct io_kiocb *req, s32 res,
 	spin_lock(&ctx->completion_lock);
 
 	if (!(req->flags & REQ_F_CQE_SKIP)) {
-		io_fill_cqe32_req(req, res, cflags, extra1, extra2);
+		__io_fill_cqe32_req(req, res, cflags, extra1, extra2);
 		io_commit_cqring(ctx);
 		posted = true;
 	}
@@ -2196,6 +2202,21 @@ static inline void __io_req_complete(struct io_kiocb *req, unsigned issue_flags,
 		io_req_complete_post(req, res, cflags);
 }
 
+static inline void __io_req_complete32(struct io_kiocb *req,
+				       unsigned int issue_flags, s32 res,
+				       u32 cflags, u64 extra1, u64 extra2)
+{
+	if (issue_flags & IO_URING_F_COMPLETE_DEFER) {
+		req->result = res;
+		req->cflags = cflags;
+		req->extra1 = extra1;
+		req->extra2 = extra2;
+		req->flags |= REQ_F_COMPLETE_INLINE;
+	} else {
+		io_req_complete_post32(req, res, cflags, extra1, extra2);
+	}
+}
+
 static inline void io_req_complete(struct io_kiocb *req, s32 res)
 {
 	__io_req_complete(req, 0, res, 0);
@@ -2753,8 +2774,15 @@ static void __io_submit_flush_completions(struct io_ring_ctx *ctx)
 			struct io_kiocb *req = container_of(node, struct io_kiocb,
 						    comp_list);
 
-			if (!(req->flags & REQ_F_CQE_SKIP))
+			if (req->flags & REQ_F_CQE_SKIP)
+				continue;
+			if (ctx->flags & IORING_SETUP_CQE32) {
+				__io_fill_cqe32_req(req, req->result,
+						    req->cflags, req->extra1,
+						    req->extra2);
+			} else {
 				__io_fill_cqe_req(req, req->result, req->cflags);
+			}
 		}
 
 		io_commit_cqring(ctx);
-- 
2.30.2

