From d5c31a8d2889eb5b01c5cd0720864a199a98579f Mon Sep 17 00:00:00 2001
From: Stefan Roesch <shr@fb.com>
Date: Tue, 12 Apr 2022 12:30:20 -0700
Subject: [PATCH v3 3/9] io_uring: setup and complete processing for CQE32

This adds the setup and completion processing for large CQE's.

Co-developed-by: Jens Axboe <axboe@kernel.dk>
Signed-off-by: Stefan Roesch <shr@fb.com>
---
 fs/io_uring.c | 63 +++++++++++++++++++++++++++++++++++++++++++++++----
 1 file changed, 59 insertions(+), 4 deletions(-)

diff --git a/fs/io_uring.c b/fs/io_uring.c
index 4ddd48a9f232..cec6fc861620 100644
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@ -2078,18 +2078,46 @@ static noinline bool io_fill_cqe_aux(struct io_ring_ctx *ctx, u64 user_data,
 	return __io_fill_cqe(ctx, user_data, res, cflags);
 }
 
-static void __io_req_complete_post(struct io_kiocb *req, s32 res,
-				   u32 cflags)
+static void io_fill_cqe32_req(struct io_kiocb *req, s32 res, u32 cflags,
+			      u64 extra1, u64 extra2)
 {
 	struct io_ring_ctx *ctx = req->ctx;
+	struct io_uring_cqe *cqe;
 
-	if (!(req->flags & REQ_F_CQE_SKIP))
-		__io_fill_cqe_req(req, res, cflags);
+	if (WARN_ON_ONCE(!(ctx->flags & IORING_SETUP_CQE32)))
+		return;
+	if (req->flags & REQ_F_CQE_SKIP)
+		return;
+
+	trace_io_uring_complete(ctx, req, req->user_data, res, cflags);
+
+	/*
+	 * If we can't get a cq entry, userspace overflowed the
+	 * submission (by quite a lot). Increment the overflow count in
+	 * the ring.
+	 */
+	cqe = io_get_cqe(ctx);
+	if (likely(cqe)) {
+		WRITE_ONCE(cqe->user_data, req->user_data);
+		WRITE_ONCE(cqe->res, res);
+		WRITE_ONCE(cqe->flags, cflags);
+		WRITE_ONCE(cqe->b[0].extra1, extra1);
+		WRITE_ONCE(cqe->b[0].extra2, extra2);
+		return;
+	}
+
+	io_account_cq_overflow(ctx);
+}
+
+static void __io_req_complete_put(struct io_kiocb *req)
+{
 	/*
 	 * If we're the last reference to this request, add to our locked
 	 * free_list cache.
 	 */
 	if (req_ref_put_and_test(req)) {
+		struct io_ring_ctx *ctx = req->ctx;
+
 		if (req->flags & (REQ_F_LINK | REQ_F_HARDLINK)) {
 			if (req->flags & IO_DISARM_MASK)
 				io_disarm_next(req);
@@ -2112,6 +2140,33 @@ static void __io_req_complete_post(struct io_kiocb *req, s32 res,
 	}
 }
 
+static void __io_req_complete_post(struct io_kiocb *req, s32 res,
+				   u32 cflags)
+{
+	if (!(req->flags & REQ_F_CQE_SKIP))
+		__io_fill_cqe_req(req, res, cflags);
+	__io_req_complete_put(req);
+}
+
+static void io_req_complete_post32(struct io_kiocb *req, s32 res,
+				   u32 cflags, u64 extra1, u64 extra2)
+{
+	struct io_ring_ctx *ctx = req->ctx;
+	bool posted = false;
+
+	spin_lock(&ctx->completion_lock);
+
+	if (!(req->flags & REQ_F_CQE_SKIP)) {
+		io_fill_cqe32_req(req, res, cflags, extra1, extra2);
+		io_commit_cqring(ctx);
+		posted = true;
+	}
+	__io_req_complete_put(req);
+	spin_unlock(&ctx->completion_lock);
+	if (posted)
+		io_cqring_ev_posted(ctx);
+}
+
 static void io_req_complete_post(struct io_kiocb *req, s32 res,
 				 u32 cflags)
 {
-- 
2.30.2

